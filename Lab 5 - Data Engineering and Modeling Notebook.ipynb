{"cells":[{"cell_type":"markdown","source":["# Data Engineering and Modeling with PySpark\n","This notebook serves as Part 1 of Lab 5: Data Engineering in Fabric Notebooks. The goal is to demonstrate the foundational steps of data engineering using PySpark, leading to the creation of a Delta table. We will explore different methods of reading data into a DataFrame and how to transform this data effectively."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e137e07c-3013-4604-9363-4dfd30eff83d"},{"cell_type":"markdown","source":["## Reading Data into DataFrames\n","This section illustrates different methods to read data into a Spark DataFrame. Understanding these methods is crucial as DataFrames serve as the core structure for data manipulation in Spark. Each method, while yielding the same result, offers different approaches that can be utilized based on the specific requirements of your data processing task."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"cba82e5c-9002-4c70-96f8-49accf91edbb"},{"cell_type":"markdown","source":["### Using Spark SQL\n","Here, we use Spark SQL to load data into a DataFrame. This method is particularly useful if you are comfortable with SQL syntax. It allows you to leverage the power of SQL queries within the Spark environment.\n","\n","Take note of the multi-part qualifier being used in the SELECT statement. \"Lakehouse_WC\" refers to a specific Lakehouse in our Fabric environment.\n","\n","```python\n","# Load data into a DataFrame using SparkSQL\n","df = spark.sql(\"SELECT * FROM Lakehouse_WC.packagetypes\")\n","\n","# Display the DataFrame\n","df.show()"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f5270240-e41b-4edd-a953-b8b79f27d333"},{"cell_type":"code","source":["# Add your code in this cell\n","# You can use the markdown cell above for reference\n","# Remember to change the name of the Lakehouse (Lakehouse_{your initials})\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"73219191-9347-4a14-9d40-f3a8e23e889d","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-14T20:29:55.9087583Z","session_start_time":null,"execution_start_time":"2024-01-14T20:29:56.1857036Z","execution_finish_time":"2024-01-14T20:29:57.7629747Z","parent_msg_id":"29832c51-719e-4870-b872-d944194f05f5"},"text/plain":"StatementMeta(, 73219191-9347-4a14-9d40-f3a8e23e889d, 4, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------------+---------------+------------+-------------------+--------------------+\n|PackageTypeID|PackageTypeName|LastEditedBy|          ValidFrom|             ValidTo|\n+-------------+---------------+------------+-------------------+--------------------+\n|            1|            Bag|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            2|          Block|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            3|         Bottle|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            4|            Box|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            5|            Can|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            6|         Carton|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            7|           Each|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            8|             Kg|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            9|         Packet|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|           10|           Pair|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|           11|         Pallet|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|           12|           Tray|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|           13|           Tub |           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|           14|           Tube|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n+-------------+---------------+------------+-------------------+--------------------+\n\n"]}],"execution_count":2,"metadata":{},"id":"9ba568a9-7760-4591-a0b5-c506f89b614b"},{"cell_type":"markdown","source":["### Using PySpark DataFrame API\n","Alternatively, we can use the PySpark DataFrame API to achieve the same result. This approach is more native to Spark and utilizes the DataFrame API's methods for data manipulation. \n","\n","```python\n","# Load the data into a DataFrame using PySpark DataFrame API\n","df = spark.table(\"Lakehouse_WC.packagetypes\")\n","\n","# Show the DataFrame\n","df.show()"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"cc6889fd-a8e2-4462-bb94-bae7d1413948"},{"cell_type":"code","source":["# Add your code in this cell\n","# You can use the markdown cell above for reference\n","# Remember to change the name of the Lakehouse (Lakehouse_{your initials})\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"73219191-9347-4a14-9d40-f3a8e23e889d","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-14T20:31:33.0317804Z","session_start_time":null,"execution_start_time":"2024-01-14T20:31:33.3554887Z","execution_finish_time":"2024-01-14T20:31:49.8499843Z","parent_msg_id":"f57d0ff3-cf7d-4bd8-8347-803a74799748"},"text/plain":"StatementMeta(, 73219191-9347-4a14-9d40-f3a8e23e889d, 5, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------------+---------------+------------+-------------------+--------------------+\n|PackageTypeID|PackageTypeName|LastEditedBy|          ValidFrom|             ValidTo|\n+-------------+---------------+------------+-------------------+--------------------+\n|            1|            Bag|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            2|          Block|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            3|         Bottle|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            4|            Box|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            5|            Can|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            6|         Carton|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            7|           Each|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            8|             Kg|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|            9|         Packet|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|           10|           Pair|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|           11|         Pallet|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|           12|           Tray|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|           13|           Tub |           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n|           14|           Tube|           1|2013-01-01 00:00:00|9999-12-31 23:59:...|\n+-------------+---------------+------------+-------------------+--------------------+\n\n"]}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"04fc3eb2-c05a-4db9-9b60-3dac9907b89a"},{"cell_type":"markdown","source":["## Building the Package Types Silver Table Using PySpark\n","Moving forward, we will demonstrate how to build a specific data structure - in this case, a silver table for package types. This involves cleansing and preparing the data to meet the requirements of a dimensional model, commonly used in data warehousing and analytics."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ddd63adc-3e1a-4dfa-8651-dad19371c71e"},{"cell_type":"markdown","source":["### Utilizing DataFrame API with Method Chaining\n","This example showcases the use of method chaining in PySpark, which enhances readability and maintains a clear flow of data transformations.\n","\n","```python\n","# Create a DataFrame using the DataFrame API with chaining format\n","distinct_df = (\n","    spark.table(\"Lakehouse_WC.PackageTypes\")  # Reads the \"PackageTypes\" table\n","    .select(\"PackageTypeId\", \"PackageTypeName\")  # Selects the specified columns\n","    .distinct()  # Retrieves distinct rows\n",")\n","\n","# Show the result\n","distinct_df.show()"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6f8225d1-31c8-4e00-90eb-f4548134d5fb"},{"cell_type":"code","source":["# Add your code in this cell\n","# You can use the markdown cell above for reference\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"73219191-9347-4a14-9d40-f3a8e23e889d","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-14T20:34:09.904784Z","session_start_time":null,"execution_start_time":"2024-01-14T20:34:10.1981078Z","execution_finish_time":"2024-01-14T20:34:12.6673846Z","parent_msg_id":"a62790fc-c72e-4c85-9a3f-1e79d55cbf10"},"text/plain":"StatementMeta(, 73219191-9347-4a14-9d40-f3a8e23e889d, 7, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------------+---------------+\n|PackageTypeId|PackageTypeName|\n+-------------+---------------+\n|            8|             Kg|\n|            4|            Box|\n|            7|           Each|\n|           14|           Tube|\n|            1|            Bag|\n|            3|         Bottle|\n|            5|            Can|\n|           11|         Pallet|\n|            6|         Carton|\n|           12|           Tray|\n|           13|           Tub |\n|           10|           Pair|\n|            2|          Block|\n|            9|         Packet|\n+-------------+---------------+\n\n"]}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"d9936377-f923-4bac-bc04-dcb10ed295c8"},{"cell_type":"markdown","source":["### Creating a Delta Table\n","Finally, we write our transformed DataFrame to a Delta table. Delta tables provide advanced capabilities like ACID transactions, scalable metadata handling, and unifies streaming and batch data processing.\n","\n","```python\n","# Write the DataFrame to the Delta table\n","distinct_df.write.format(\"delta\").saveAsTable(\"Silver_Lakehouse_WC.package_type\")"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9857c1b0-c8b2-4377-b339-bba2a3ade154"},{"cell_type":"code","source":["# Add your code in this cell\n","# You can use the markdown cell above for reference\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"73219191-9347-4a14-9d40-f3a8e23e889d","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-14T20:35:18.165069Z","session_start_time":null,"execution_start_time":"2024-01-14T20:35:18.4524016Z","execution_finish_time":"2024-01-14T20:35:28.6089463Z","parent_msg_id":"24595df3-9c3d-449e-84fd-1c10cbc3f561"},"text/plain":"StatementMeta(, 73219191-9347-4a14-9d40-f3a8e23e889d, 8, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9ae15b1a-7b56-471d-b6fd-10092cd823e9"},{"cell_type":"markdown","source":["## Data Transformation with Spark SQL\n","Spark SQL integrates seamlessly with PySpark, allowing the execution of SQL queries. Multi-line SQL queries can be neatly encapsulated within triple quotes (`\"\"\"`) in Python. This practice improves the readability of complex SQL statements.\n","\n","```python\n","# Use Spark SQL for complex data transformation\n","# Triple quotes are used for multi-line SQL queries\n","sales_orders_df = spark.sql(\"\"\"\n","SELECT \n","    a.OrderLineId,\n","    a.OrderId,\n","    a.StockItemId,\n","    a.PackageTypeId,\n","    b.CustomerId,\n","    b.SalespersonPersonId,\n","    c.DeliveryCityId,\n","    c.BillToCustomerId,\n","    b.OrderDate,\n","    b.ExpectedDeliveryDate,\n","    a.Quantity,\n","    a.UnitPrice,\n","    a.TaxRate,\n","    a.PickedQuantity\n","FROM \n","    Lakehouse_WC.Sales_OrderLines a\n","JOIN \n","    Lakehouse_WC.Sales_Orders b ON a.OrderId = b.OrderId\n","JOIN \n","    Lakehouse_WC.Sales_Customers c ON b.CustomerId = c.CustomerId\n","\"\"\")\n","\n","# Display a sample of the DataFrame\n","sales_orders_df.show(5)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"35a0977c-d94e-4dbd-8e73-63d1b14a72a5"},{"cell_type":"code","source":["# Add your code in this cell\n","# You can use the markdown cell above for reference\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"73219191-9347-4a14-9d40-f3a8e23e889d","statement_id":10,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-14T20:38:44.9736359Z","session_start_time":null,"execution_start_time":"2024-01-14T20:38:45.2417759Z","execution_finish_time":"2024-01-14T20:38:51.6098275Z","parent_msg_id":"c1498c57-5009-4255-aaa0-c9836e66daed"},"text/plain":"StatementMeta(, 73219191-9347-4a14-9d40-f3a8e23e889d, 10, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-----------+-------+-----------+-------------+----------+-------------------+--------------+----------------+----------+--------------------+--------+---------+-------+--------------+\n|OrderLineId|OrderId|StockItemId|PackageTypeId|CustomerId|SalespersonPersonId|DeliveryCityId|BillToCustomerId| OrderDate|ExpectedDeliveryDate|Quantity|UnitPrice|TaxRate|PickedQuantity|\n+-----------+-------+-----------+-------------+----------+-------------------+--------------+----------------+----------+--------------------+--------+---------+-------+--------------+\n|      25168|   7903|        141|           10|       940|                 14|         27543|             940|2020-05-30|          2020-05-31|      24|     5.00| 15.000|            24|\n|      65871|  20750|        141|           10|       988|                  6|         19979|             988|2021-01-24|          2021-01-27|      24|     5.00| 15.000|            24|\n|      97458|  30762|        141|           10|        75|                 15|         37473|               1|2021-07-14|          2021-07-15|      24|     5.00| 15.000|            24|\n|     103612|  32728|        141|           10|       482|                 13|          5664|             401|2021-08-15|          2021-08-18|      24|     5.00| 15.000|            24|\n|     107971|  34118|        141|           10|       591|                 14|         16382|             401|2021-09-11|          2021-09-12|      24|     5.00| 15.000|            24|\n+-----------+-------+-----------+-------------+----------+-------------------+--------------+----------------+----------+--------------------+--------+---------+-------+--------------+\nonly showing top 5 rows\n\n"]}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"501d42e1-59f3-4c1c-9f79-84ff5b76131f"},{"cell_type":"markdown","source":["### Writing Data to a Delta Table\n","Writing the DataFrame to a Delta table is the next step. Delta tables provide a more advanced format for storage and querying within Spark. This step demonstrates the process of persisting a DataFrame as a Delta table, a common practice in data engineering for building reliable data pipelines.\n","\n","```python\n","# Persist the DataFrame as a Delta table\n","sales_orders_df.write.format(\"delta\").saveAsTable(\"Silver_Lakehouse_WC.sales_orders\")"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"11b2e354-411e-412f-ad91-794214009226"},{"cell_type":"code","source":["# Add your code in this cell\n","# You can use the markdown cell above for reference\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"73219191-9347-4a14-9d40-f3a8e23e889d","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2024-01-14T20:39:47.3427615Z","session_start_time":null,"execution_start_time":"2024-01-14T20:39:47.6239793Z","execution_finish_time":"2024-01-14T20:39:54.0863983Z","parent_msg_id":"ad6e5008-9437-4a4c-b220-4576fb7c64b7"},"text/plain":"StatementMeta(, 73219191-9347-4a14-9d40-f3a8e23e889d, 11, Finished, Available)"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"aa36cf31-4b44-4ea6-8b90-ca582ae0678e"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"trident":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}